{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read a file from HDFS and count the words #\n",
    "This is a simple PySpark program which reads a file from a remote Hadoop filesystem and counts the words it contains. To run this example you need a spark cluster running in your project, and you need to know the host and port for a reachable Namenode in an external Hadoop cluster.\n",
    "\n",
    "Note, this example will run as the user *nbuser*, so the HDFS filesystem path needs to be readable by *nbuser* on the Hadoop cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Connect to the Spark cluster ##\n",
    "\n",
    "This example assumes that a Spark cluster is already running in your project. Change *spark://mycluster:7077* to the URL of the Spark master in the block below. Running this block creates the SparkContext, so you only want to run it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "spc = SparkContext(\"spark://mycluster:7077\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set the HDFS host, port, and input path ##\n",
    "The *hdfs_host* value should be the hostname of the machine running your hadoop Namenode, and the *hdfs_port* should be the port that the Namenode is listening on.  The default port is 8020. If a different port has been set, it can be found in the *fs.defaultFS* property in the *core-site.xml* configuration file on your Hadoop cluster.\n",
    "\n",
    "The *hdfs* path should be the absolute path of the file that you want to read from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hdfs_host = \"myhost.me.com\"\n",
    "hdfs_port = 8020\n",
    "hdfs_path = \"/user/me/input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Read the file and print the counts for  up to 20 words ##\n",
    "This block reads the file and splits up words on the space character. It turns the resulting RDD into a list using *collect()* and then prints up to 20 key/value pairs showing the key and the number of occurrences for each.\n",
    "\n",
    "The second line in this block is the most important -- this is where Spark actually reads the file from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "text_file = spc.textFile(\"hdfs://%s:%d%s\" % (hdfs_host, hdfs_port, os.path.join(\"/\", hdfs_path)))\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "values = counts.collect()\n",
    "if len(values) > 20:\n",
    "    values = values[:20]\n",
    "print(\"\\n\".join([x[0] + \" = \" + str(x[1]) for x in values]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
